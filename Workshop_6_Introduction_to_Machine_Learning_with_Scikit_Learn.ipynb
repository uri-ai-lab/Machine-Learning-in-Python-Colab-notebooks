{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uri-ai-lab/Machine-Learning-in-Python-Colab-notebooks/blob/main/Workshop_6_Introduction_to_Machine_Learning_with_Scikit_Learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Machine Learning with Scikit-Learn"
      ],
      "metadata": {
        "id": "guXgMGSEPIic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categories of Machine Learning\n",
        "\n",
        "At the most fundamental level, machine learning can be categorized into two main types: supervised learning and unsupervised learning.\n",
        "\n",
        "Supervised learning involves somehow modeling the relationship between measured features of data and some label associated with the data; once this model is determined, it can be used to apply labels to new, unknown data. This is further subdivided into classification tasks and regression tasks: in classification, the labels are discrete categories, while in regression, the labels are continuous quantities. We will see examples of both types of supervised learning.\n",
        "\n",
        "Unsupervised learning involves modeling the features of a dataset without reference to any label, and is often described as \"letting the dataset speak for itself.\" These models include tasks such as clustering and dimensionality reduction. Clustering algorithms identify distinct groups of data, while dimensionality reduction algorithms search for more succinct representations of the data.\n",
        "\n",
        "In addition, there are so-called semi-supervised learning methods, which falls somewhere between supervised learning and unsupervised learning. Semi-supervised learning methods are often useful when only incomplete labels are available."
      ],
      "metadata": {
        "id": "1U2p3kmcPQwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In short,\n",
        "\n",
        "- *Supervised learning*: Models that can predict labels based on labeled training data\n",
        "\n",
        "  - *Classification*: Models that predict labels as two or more discrete categories\n",
        "  - *Regression*: Models that predict continuous labels\n",
        "\n",
        "- *Unsupervised learning*: Models that identify structure in unlabeled data\n",
        "\n",
        "  - *Clustering*: Models that detect and identify distinct groups in the data\n",
        "  - *Dimensionality reduction*: Models that detect and identify lower-dimensional structure in higher-dimensional data"
      ],
      "metadata": {
        "id": "Xl01vW1HPxO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing Scikit-Learn\n",
        "\n",
        "There are several Python libraries which provide solid implementations of a range of machine learning algorithms.\n",
        "One of the best known is [Scikit-Learn](http://scikit-learn.org), a package that provides efficient versions of a large number of common algorithms.\n",
        "Scikit-Learn is characterized by a clean, uniform, and streamlined API, as well as by very useful and complete online documentation.\n"
      ],
      "metadata": {
        "id": "lv9Kf_u2V65v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basics of the API\n",
        "\n",
        "Most commonly, the steps in using the Scikit-Learn estimator API are as follows\n",
        "(we will step through a handful of detailed examples in the sections that follow).\n",
        "\n",
        "1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn.\n",
        "2. Choose model hyperparameters by instantiating this class with desired values.\n",
        "3. Arrange data into a features matrix and target vector.\n",
        "4. Fit the model to your data by calling the ``fit()`` method of the model instance.\n",
        "5. Apply the Model to new data:\n",
        "   - For supervised learning, often we predict labels for unknown data using the ``predict()`` method.\n",
        "   - For unsupervised learning, we often transform or infer properties of the data using the ``transform()`` or ``predict()`` method."
      ],
      "metadata": {
        "id": "lilf_NrgWm1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised learning examples"
      ],
      "metadata": {
        "id": "zIyuIKcaTyOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple linear regression"
      ],
      "metadata": {
        "id": "GK0TUNUSTrri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example of this process, let's consider a simple linear regression‚Äîthat is, the common case of fitting a line to  (ùë•,ùë¶)  data. We will use the following simple data for our regression example:"
      ],
      "metadata": {
        "id": "x6KN-7jpT7bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "rng = np.random.RandomState(42)\n",
        "x = 10 * rng.rand(50)\n",
        "y = 2 * x - 1 + rng.randn(50)\n",
        "plt.scatter(x, y);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "9-GCk3ejT0Rh",
        "outputId": "68248037-4eef-4f11-8fd8-ad6a697d7b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "nyStciQPTz3_"
      },
      "source": [
        "With this data in place, we can use the recipe outlined earlier. Let's walk through the process:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "dHNNkSCKTz4A"
      },
      "source": [
        "#### 1. Choose a class of model\n",
        "\n",
        "In Scikit-Learn, every class of model is represented by a Python class.\n",
        "So, for example, if we would like to compute a simple linear regression model, we can import the linear regression class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "UXb_RsXvTz4A"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "WSKf3WGGTz4A"
      },
      "source": [
        "Note that other more general linear regression models exist as well; you can read more about them in the [``sklearn.linear_model`` module documentation](http://Scikit-Learn.org/stable/modules/linear_model.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Choose model hyperparameters\n",
        "\n",
        "For our linear regression example, we can instantiate the ``LinearRegression`` class and specify that we would like to fit the intercept using the ``fit_intercept`` hyperparameter:"
      ],
      "metadata": {
        "id": "7gOFEe5CYBeL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "L0eGlLsOTz4A",
        "outputId": "a5834d56-2e81-42fc-b56d-6e5f14cbe112",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model = LinearRegression(fit_intercept=True)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfRw6HIiBBaY",
        "outputId": "1fc447fe-f8c2-44c3-d76b-a5c5baa0cf29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.74540119, 9.50714306, 7.31993942, 5.98658484, 1.5601864 ,\n",
              "       1.5599452 , 0.58083612, 8.66176146, 6.01115012, 7.08072578,\n",
              "       0.20584494, 9.69909852, 8.32442641, 2.12339111, 1.81824967,\n",
              "       1.8340451 , 3.04242243, 5.24756432, 4.31945019, 2.9122914 ,\n",
              "       6.11852895, 1.39493861, 2.92144649, 3.66361843, 4.56069984,\n",
              "       7.85175961, 1.99673782, 5.14234438, 5.92414569, 0.46450413,\n",
              "       6.07544852, 1.70524124, 0.65051593, 9.48885537, 9.65632033,\n",
              "       8.08397348, 3.04613769, 0.97672114, 6.84233027, 4.40152494,\n",
              "       1.22038235, 4.9517691 , 0.34388521, 9.09320402, 2.58779982,\n",
              "       6.62522284, 3.11711076, 5.20068021, 5.46710279, 1.84854456])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "KCCJh5lfTz4A"
      },
      "source": [
        "#### 3. Arrange data into a features matrix and target vector\n",
        "\n",
        "The Scikit-Learn data representation requires a two-dimensional features matrix and a one-dimensional target array.\n",
        "Here our target variable ``y`` is already in the correct form (a length-``n_samples`` array), but we need to massage the data ``x`` to make it a matrix of size ``[n_samples, n_features]``.\n",
        "In this case, this amounts to a simple reshaping of the one-dimensional array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "-hNMmNM-Tz4A",
        "outputId": "a14d9d65-d7df-4b1b-aee9-6dad867acc39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.74540119],\n",
              "       [9.50714306],\n",
              "       [7.31993942],\n",
              "       [5.98658484],\n",
              "       [1.5601864 ],\n",
              "       [1.5599452 ],\n",
              "       [0.58083612],\n",
              "       [8.66176146],\n",
              "       [6.01115012],\n",
              "       [7.08072578],\n",
              "       [0.20584494],\n",
              "       [9.69909852],\n",
              "       [8.32442641],\n",
              "       [2.12339111],\n",
              "       [1.81824967],\n",
              "       [1.8340451 ],\n",
              "       [3.04242243],\n",
              "       [5.24756432],\n",
              "       [4.31945019],\n",
              "       [2.9122914 ],\n",
              "       [6.11852895],\n",
              "       [1.39493861],\n",
              "       [2.92144649],\n",
              "       [3.66361843],\n",
              "       [4.56069984],\n",
              "       [7.85175961],\n",
              "       [1.99673782],\n",
              "       [5.14234438],\n",
              "       [5.92414569],\n",
              "       [0.46450413],\n",
              "       [6.07544852],\n",
              "       [1.70524124],\n",
              "       [0.65051593],\n",
              "       [9.48885537],\n",
              "       [9.65632033],\n",
              "       [8.08397348],\n",
              "       [3.04613769],\n",
              "       [0.97672114],\n",
              "       [6.84233027],\n",
              "       [4.40152494],\n",
              "       [1.22038235],\n",
              "       [4.9517691 ],\n",
              "       [0.34388521],\n",
              "       [9.09320402],\n",
              "       [2.58779982],\n",
              "       [6.62522284],\n",
              "       [3.11711076],\n",
              "       [5.20068021],\n",
              "       [5.46710279],\n",
              "       [1.84854456]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "X = x[:, np.newaxis]\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "es5-L0GKTz4A"
      },
      "source": [
        "#### 4. Fit the model to your data\n",
        "\n",
        "Now it is time to apply our model to data.\n",
        "This can be done with the ``fit()`` method of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5D21BAGyTz4A",
        "outputId": "55238989-61db-4357-9b17-527caa726571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UORc88NkTz4B"
      },
      "source": [
        "This ``fit()`` command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributes that the user can explore.\n",
        "In Scikit-Learn, by convention all model parameters that were learned during the ``fit()`` process have trailing underscores; for example in this linear model, we have the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "4flukdqVTz4B",
        "outputId": "cd30707c-75d9-42e5-9781-9df4fcfb86c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.9776566])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "s3YH2dRDTz4B",
        "outputId": "a59e7f46-e695-4437-c65a-67e078696c16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.9033107255311146"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "zGb-dL7qTz4B"
      },
      "source": [
        "These two parameters represent the slope and intercept of the simple linear fit to the data.\n",
        "Comparing to the data definition, we see that they are very close to the input slope of 2 and intercept of -1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1Ye74mW9Tz4B"
      },
      "source": [
        "#### 5. Predict labels for unknown data\n",
        "\n",
        "Once the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was not part of the training set.\n",
        "In Scikit-Learn, this can be done using the ``predict()`` method.\n",
        "For the sake of this example, our \"new data\" will be a grid of *x* values, and we will ask what *y* values the model predicts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "O1V3Ds3hTz4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50989746-dbd0-4304-ba5c-b0c2c8c1a2b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.        , -0.75510204, -0.51020408, -0.26530612, -0.02040816,\n",
              "        0.2244898 ,  0.46938776,  0.71428571,  0.95918367,  1.20408163,\n",
              "        1.44897959,  1.69387755,  1.93877551,  2.18367347,  2.42857143,\n",
              "        2.67346939,  2.91836735,  3.16326531,  3.40816327,  3.65306122,\n",
              "        3.89795918,  4.14285714,  4.3877551 ,  4.63265306,  4.87755102,\n",
              "        5.12244898,  5.36734694,  5.6122449 ,  5.85714286,  6.10204082,\n",
              "        6.34693878,  6.59183673,  6.83673469,  7.08163265,  7.32653061,\n",
              "        7.57142857,  7.81632653,  8.06122449,  8.30612245,  8.55102041,\n",
              "        8.79591837,  9.04081633,  9.28571429,  9.53061224,  9.7755102 ,\n",
              "       10.02040816, 10.26530612, 10.51020408, 10.75510204, 11.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "xfit = np.linspace(-1, 11)\n",
        "xfit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hGpLLIuVTz4B"
      },
      "source": [
        "As before, we need to coerce these *x* values into a ``[n_samples, n_features]`` features matrix, after which we can feed it to the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GJ0xRlnpTz4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5538d32f-7a3b-451e-e040-8dfe2623454f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.88096733, -2.39664326, -1.9123192 , -1.42799513, -0.94367106,\n",
              "       -0.459347  ,  0.02497707,  0.50930113,  0.9936252 ,  1.47794926,\n",
              "        1.96227333,  2.44659739,  2.93092146,  3.41524552,  3.89956959,\n",
              "        4.38389366,  4.86821772,  5.35254179,  5.83686585,  6.32118992,\n",
              "        6.80551398,  7.28983805,  7.77416211,  8.25848618,  8.74281024,\n",
              "        9.22713431,  9.71145837, 10.19578244, 10.68010651, 11.16443057,\n",
              "       11.64875464, 12.1330787 , 12.61740277, 13.10172683, 13.5860509 ,\n",
              "       14.07037496, 14.55469903, 15.03902309, 15.52334716, 16.00767122,\n",
              "       16.49199529, 16.97631936, 17.46064342, 17.94496749, 18.42929155,\n",
              "       18.91361562, 19.39793968, 19.88226375, 20.36658781, 20.85091188])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "Xfit = xfit[:, np.newaxis]\n",
        "yfit = model.predict(Xfit)\n",
        "yfit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "v5yzMQq3Tz4C"
      },
      "source": [
        "Finally, let's visualize the results by plotting first the raw data, and then this model fit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3jYwgp-ATz4C",
        "outputId": "0b229253-7dd1-43d3-ba19-f75c668d97eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.scatter(x, y)\n",
        "plt.plot(xfit, yfit);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_k0pbRiPTz4C"
      },
      "source": [
        "Typically the efficacy of the model is evaluated by comparing its results to some known baseline, as we will see in the next example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ],
      "metadata": {
        "id": "5F2CoSDcfw9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at another example of this process, using the Iris dataset we discussed last week.\n",
        "Our question will be this: given a model trained on a portion of the Iris data, how well can we predict the remaining labels?\n",
        "\n",
        "For this task, we will use an extremely simple generative model known as Gaussian naive Bayes, which proceeds by assuming each class is drawn from an axis-aligned Gaussian distribution (see [In Depth: Naive Bayes Classification](05.05-Naive-Bayes.ipynb) for more details).\n",
        "Because it is so fast and has no hyperparameters to choose, Gaussian naive Bayes is often a good model to use as a baseline classification, before exploring whether improvements can be found through more sophisticated models."
      ],
      "metadata": {
        "id": "WuW9cE3OgScE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First load the dataset to dataframe. We can download this dataset in the form of a Pandas ``DataFrame`` using the [seaborn](http://seaborn.pydata.org/) library:"
      ],
      "metadata": {
        "id": "65FgW4dwhTeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "iris = sns.load_dataset('iris')\n",
        "iris.head()"
      ],
      "metadata": {
        "id": "TNfEwIs7gkd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "8lwl00N5Tz38"
      },
      "source": [
        "Here each row of the data refers to a single observed flower, and the number of rows is the total number of flowers in the dataset.\n",
        "In general, we will refer to the rows of the matrix as *samples*, and the number of rows as ``n_samples``.\n",
        "\n",
        "Likewise, each column of the data refers to a particular quantitative piece of information that describes each sample.\n",
        "In general, we will refer to the columns of the matrix as *features*, and the number of columns as ``n_features``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "4JOA5SouTz38"
      },
      "source": [
        "#### Features matrix\n",
        "\n",
        "This table layout makes clear that the information can be thought of as a two-dimensional numerical array or matrix, which we will call the *features matrix*.\n",
        "By convention, this features matrix is often stored in a variable named ``X``.\n",
        "The features matrix is assumed to be two-dimensional, with shape ``[n_samples, n_features]``, and is most often contained in a NumPy array or a Pandas ``DataFrame``, though some Scikit-Learn models also accept SciPy sparse matrices.\n",
        "\n",
        "The samples (i.e., rows) always refer to the individual objects described by the dataset.\n",
        "For example, the sample might be a flower, a person, a document, an image, a sound file, a video, an astronomical object, or anything else you can describe with a set of quantitative measurements.\n",
        "\n",
        "The features (i.e., columns) always refer to the distinct observations that describe each sample in a quantitative manner.\n",
        "Features are generally real-valued, but may be Boolean or discrete-valued in some cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "EoLhLwrcTz39"
      },
      "source": [
        "#### Target array\n",
        "\n",
        "In addition to the feature matrix ``X``, we also generally work with a *label* or *target* array, which by convention we will usually call ``y``.\n",
        "The target array is usually one dimensional, with length ``n_samples``, and is generally contained in a NumPy array or Pandas ``Series``.\n",
        "The target array may have continuous numerical values, or discrete classes/labels.\n",
        "While some Scikit-Learn estimators do handle multiple target values in the form of a two-dimensional, ``[n_samples, n_targets]`` target array, we will primarily be working with the common case of a one-dimensional target array.\n",
        "\n",
        "Often one point of confusion is how the target array differs from the other features columns. The distinguishing feature of the target array is that it is usually the quantity we want to *predict from the data*: in statistical terms, it is the dependent variable.\n",
        "For example, in the preceding data we may wish to construct a model that can predict the species of flower based on the other measurements; in this case, the ``species`` column would be considered the target array.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Q2L4yNGFTz39"
      },
      "source": [
        "For use in Scikit-Learn, we will extract the features matrix and target array from the ``DataFrame``, which we can do using some of the Pandas ``DataFrame`` operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "gN9v6lFHTz39"
      },
      "outputs": [],
      "source": [
        "X_iris = iris.drop('species', axis=1)\n",
        "X_iris.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UcObbR90Tz3-"
      },
      "outputs": [],
      "source": [
        "y_iris = iris['species']\n",
        "y_iris.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BtIVJvBGTz3-"
      },
      "source": [
        "To summarize, the expected layout of features and target values is visualized in the following diagram:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "bNJ-0wz8Tz3-"
      },
      "source": [
        "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.02-samples-features.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would like to evaluate the model on data it has not seen before, and so we will split the data into a training set and a testing set. This could be done by hand, but it is more convenient to use the train_test_split utility function:"
      ],
      "metadata": {
        "id": "pezrQO9lzt5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "jjttg63_Tz4C"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris,\n",
        "                                                random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5pnCt7A-Tz4C"
      },
      "source": [
        "With the data arranged, we can follow our recipe to predict the labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2gXQVH5ITz4C"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB # 1. choose model class\n",
        "model = GaussianNB()                       # 2. instantiate model\n",
        "model.fit(Xtrain, ytrain)                  # 3. fit model to data\n",
        "y_model = model.predict(Xtest)             # 4. predict on new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "D3GpFKjSTz4C"
      },
      "source": [
        "Finally, we can use the ``accuracy_score`` utility to see the fraction of predicted labels that match their true value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "9va_PJAyTz4C"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(ytest, y_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6Brp0q6dTz4C"
      },
      "source": [
        "With an accuracy topping 97%, we see that even this very naive classification algorithm is effective for this particular dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised learning examples"
      ],
      "metadata": {
        "id": "T-kdM1hzz-ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensionality reduction"
      ],
      "metadata": {
        "id": "TUKQlJFx0LUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example of an unsupervised learning problem, let's take a look at reducing the dimensionality of the Iris data so as to more easily visualize it.\n",
        "Recall that the Iris data is four dimensional: there are four features recorded for each sample.\n",
        "\n",
        "The task of dimensionality reduction is to ask whether there is a suitable lower-dimensional representation that retains the essential features of the data.\n",
        "Often dimensionality reduction is used as an aid to visualizing data: after all, it is much easier to plot data in two dimensions than in four dimensions or higher!\n",
        "\n",
        "Here we will use principal component analysis (PCA; see [In Depth: Principal Component Analysis](05.09-Principal-Component-Analysis.ipynb)), which is a fast linear dimensionality reduction technique.\n",
        "We will ask the model to return two components‚Äîthat is, a two-dimensional representation of the data.\n",
        "\n",
        "Following the sequence of steps outlined earlier, we have:"
      ],
      "metadata": {
        "id": "cesVt5If0aL2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "jeDr2IhuTz4D"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA  # 1. Choose the model class\n",
        "model = PCA(n_components=2)            # 2. Instantiate the model with hyperparameters\n",
        "model.fit(X_iris)                      # 3. Fit to data. Notice y is not specified!\n",
        "X_2D = model.transform(X_iris)         # 4. Transform the data to two dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "0HXnYNWATz4D"
      },
      "source": [
        "Now let's plot the results. A quick way to do this is to insert the results into the original Iris ``DataFrame``, and use Seaborn's ``lmplot`` to show the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "PA2lEodjTz4D"
      },
      "outputs": [],
      "source": [
        "iris['PCA1'] = X_2D[:, 0]\n",
        "iris['PCA2'] = X_2D[:, 1]\n",
        "plt.scatter(iris.PCA1, iris.PCA2, c=iris.species.astype('category').cat.codes);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "rRB81vI5Tz4D"
      },
      "source": [
        "We see that in the two-dimensional representation, the species are fairly well separated, even though the PCA algorithm had no knowledge of the species labels!\n",
        "This indicates to us that a relatively straightforward classification will probably be effective on the dataset, as we saw before."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering"
      ],
      "metadata": {
        "id": "ocyMwjRS0w7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's next look at applying clustering to the Iris data.\n",
        "A clustering algorithm attempts to find distinct groups of data without reference to any labels.\n",
        "Here we will use a powerful clustering method called a Gaussian mixture model (GMM), discussed in more detail in [In Depth: Gaussian Mixture Models](05.12-Gaussian-Mixtures.ipynb).\n",
        "A GMM attempts to model the data as a collection of Gaussian blobs.\n",
        "\n",
        "We can fit the Gaussian mixture model as follows:"
      ],
      "metadata": {
        "id": "kfl83jzA1A5e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "qY0Ehh-fTz4D"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture      # 1. Choose the model class\n",
        "model = GaussianMixture(n_components=3,\n",
        "            covariance_type='full')  # 2. Instantiate the model with hyperparameters\n",
        "model.fit(X_iris)                    # 3. Fit to data. Notice y is not specified!\n",
        "y_gmm = model.predict(X_iris)        # 4. Determine cluster labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1-DHw14XTz4D"
      },
      "source": [
        "As before, we will add the cluster label to the Iris ``DataFrame`` and plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "yJ8O3rxUTz4D"
      },
      "outputs": [],
      "source": [
        "iris['cluster'] = y_gmm\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.set_figwidth(15)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Species\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.scatter(iris.PCA1, iris.PCA2, c = iris.species.astype('category').cat.codes)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Clusters\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.scatter(iris.PCA1, iris.PCA2, c= iris.cluster)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "zkkIp2pRTz4E"
      },
      "source": [
        "By splitting the data by cluster number, we see exactly how well the GMM algorithm has recovered the underlying label: the *setosa* species is separated perfectly within cluster 0, while there remains a small amount of mixing between *versicolor* and *virginica*.\n",
        "This means that even without an expert to tell us the species labels of the individual flowers, the measurements of these flowers are distinct enough that we could *automatically* identify the presence of these different groups of species with a simple clustering algorithm!\n",
        "This sort of algorithm might further give experts in the field clues as to the relationship between the samples they are observing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "YHMO5V0-Tz4E"
      },
      "source": [
        "## Application: Exploring Hand-written Digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "SrxfTYthTz4E"
      },
      "source": [
        "To demonstrate these principles on a more interesting problem, let's consider one piece of the optical character recognition problem: the identification of hand-written digits.\n",
        "In the wild, this problem involves both locating and identifying characters in an image. Here we'll take a shortcut and use Scikit-Learn's set of pre-formatted digits, which is built into the library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "F92-7pT_Tz4E"
      },
      "source": [
        "### Loading and visualizing the digits data\n",
        "\n",
        "We'll use Scikit-Learn's data access interface and take a look at this data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "K4-bC5vcTz4E"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "7z37ZsIiTz4E"
      },
      "source": [
        "The images data is a three-dimensional array: 1,797 samples each consisting of an 8 √ó 8 grid of pixels.\n",
        "Let's visualize the first hundred of these:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6Dza-IzjTz4E"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
        "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
        "            transform=ax.transAxes, color='green')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_eM0MgiqTz4E"
      },
      "source": [
        "In order to work with this data within Scikit-Learn, we need a two-dimensional, ``[n_samples, n_features]`` representation.\n",
        "We can accomplish this by treating each pixel in the image as a feature: that is, by flattening out the pixel arrays so that we have a length-64 array of pixel values representing each digit.\n",
        "Additionally, we need the target array, which gives the previously determined label for each digit.\n",
        "These two quantities are built into the digits dataset under the ``data`` and ``target`` attributes, respectively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "7HQKy_QoTz4E"
      },
      "outputs": [],
      "source": [
        "X = digits.data\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "0xj2JJvVTz4E"
      },
      "outputs": [],
      "source": [
        "y = digits.target\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Zj_x76pITz4E"
      },
      "source": [
        "We see here that there are 1,797 samples and 64 features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "IDbBDn1xTz4F"
      },
      "source": [
        "### Unsupervised learning: Dimensionality reduction\n",
        "\n",
        "We'd like to visualize our points within the 64-dimensional parameter space, but it's difficult to effectively visualize points in such a high-dimensional space.\n",
        "Instead we'll reduce the dimensions to 2, using an unsupervised method.\n",
        "Here, we'll make use of a manifold learning algorithm called *Isomap* (see [In-Depth: Manifold Learning](05.10-Manifold-Learning.ipynb)), and transform the data to two dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "yCccmphsTz4F"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import Isomap\n",
        "iso = Isomap(n_components=2)\n",
        "iso.fit(digits.data)\n",
        "data_projected = iso.transform(digits.data)\n",
        "data_projected.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Mlbf5buQTz4F"
      },
      "source": [
        "We see that the projected data is now two-dimensional.\n",
        "Let's plot this data to see if we can learn anything from its structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ML4_PVTJTz4F"
      },
      "outputs": [],
      "source": [
        "plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target,\n",
        "            edgecolor='none', alpha=0.5,\n",
        "            cmap=plt.cm.get_cmap('Spectral_r', 10))\n",
        "plt.colorbar(label='digit label', ticks=range(10))\n",
        "plt.clim(-0.5, 9.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "G1jBin0ITz4F"
      },
      "source": [
        "This plot gives us some good intuition into how well various numbers are separated in the larger 64-dimensional space. For example, zeros (in purple) and ones (in blue) have very little overlap in parameter space.\n",
        "Intuitively, this makes sense: a zero is empty in the middle of the image, while a one will generally have ink in the middle.\n",
        "On the other hand, there seems to be a more or less continuous spectrum between ones and fours: we can understand this by realizing that some people draw ones with \"hats\" on them, which cause them to look similar to fours.\n",
        "\n",
        "Overall, however, the different groups appear to be fairly well separated in the parameter space: this tells us that even a very straightforward supervised classification algorithm should perform suitably on this data.\n",
        "Let's give it a try."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hQQ4UpALTz4F"
      },
      "source": [
        "### Classification on digits\n",
        "\n",
        "Let's apply a classification algorithm to the digits.\n",
        "As with the Iris data previously, we will split the data into a training and testing set, and fit a Gaussian naive Bayes model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "YcYnVuQyTz4F"
      },
      "outputs": [],
      "source": [
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "ToNk8s29Tz4F"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "model.fit(Xtrain, ytrain)\n",
        "y_model = model.predict(Xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ByqUGoKCTz4F"
      },
      "source": [
        "Now that we have predicted our model, we can gauge its accuracy by comparing the true values of the test set to the predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "a2kzBYakTz4F"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(ytest, y_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "KT-3iJrFTz4F"
      },
      "source": [
        "With even this extremely simple model, we find about 80% accuracy for classification of the digits!\n",
        "However, this single number doesn't tell us *where* we've gone wrong‚Äîone nice way to do this is to use the *confusion matrix*, which we can compute with Scikit-Learn and plot with Seaborn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "aqihb0j0Tz4F"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "mat = confusion_matrix(ytest, y_model)\n",
        "\n",
        "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
        "plt.xlabel('predicted value')\n",
        "plt.ylabel('true value');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "oUT4yRvYTz4F"
      },
      "source": [
        "This shows us where the mis-labeled points tend to be: for example, a large number of twos here are mis-classified as either ones or eights.\n",
        "Another way to gain intuition into the characteristics of the model is to plot the inputs again, with their predicted labels.\n",
        "We'll use green for correct labels, and red for incorrect labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3Wmc9lyITz4G"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "\n",
        "test_images = Xtest.reshape(-1, 8, 8)\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(test_images[i], cmap='binary', interpolation='nearest')\n",
        "    ax.text(0.05, 0.05, str(y_model[i]),\n",
        "            transform=ax.transAxes,\n",
        "            color='green' if (ytest[i] == y_model[i]) else 'red')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "IH2-laSvTz4G"
      },
      "source": [
        "Examining this subset of the data, we can gain insight regarding where the algorithm might be not performing optimally.\n",
        "To go beyond our 80% classification rate, we might move to a more sophisticated algorithm such as support vector machines (see [In-Depth: Support Vector Machines](05.07-Support-Vector-Machines.ipynb)), random forests (see [In-Depth: Decision Trees and Random Forests](05.08-Random-Forests.ipynb)) or another classification approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "cL7kAkSYNOSh"
      },
      "source": [
        "## Model validation the right way: Holdout sets\n",
        "\n",
        "\n",
        "A better sense of a model's performance can be found using what's known as a *holdout set*: that is, we hold back some subset of the data from the training of the model, and then use this holdout set to check the model performance.\n",
        "This splitting can be done using the ``train_test_split`` utility in Scikit-Learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "t4qKs6NTNOSh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# split the data with 50% in each set\n",
        "X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n",
        "                                  train_size=0.5)\n",
        "\n",
        "# fit the model on one set of data\n",
        "model.fit(X1, y1)\n",
        "\n",
        "# evaluate the model on the second set of data\n",
        "y2_model = model.predict(X2)\n",
        "accuracy_score(y2, y2_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "gBmd5OELNOSi"
      },
      "source": [
        "We see here a more reasonable result: the nearest-neighbor classifier is about 90% accurate on this hold-out set.\n",
        "The hold-out set is similar to unknown data, because the model has not \"seen\" it before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "yBUbPuCeNOSi"
      },
      "source": [
        "## Model validation via cross-validation\n",
        "\n",
        "One disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the model training.\n",
        "In the above case, half the dataset does not contribute to the training of the model!\n",
        "This is not optimal, and can cause problems ‚Äì especially if the initial set of training data is small.\n",
        "\n",
        "One way to address this is to use *cross-validation*; that is, to do a sequence of fits where each subset of the data is used both as a training set and as a validation set.\n",
        "Visually, it might look something like this:\n",
        "\n",
        "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.03-2-fold-CV.png?raw=1)\n",
        "\n",
        "Here we do two validation trials, alternately using each half of the data as a holdout set.\n",
        "Using the split data from before, we could implement it like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "TVxjCXfrNOSi"
      },
      "outputs": [],
      "source": [
        "y2_model = model.fit(X1, y1).predict(X2)\n",
        "y1_model = model.fit(X2, y2).predict(X1)\n",
        "accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "dy1UXLYLNOSi"
      },
      "source": [
        "What comes out are two accuracy scores, which we could combine (by, say, taking the mean) to get a better measure of the global model performance.\n",
        "This particular form of cross-validation is a *two-fold cross-validation*‚Äîthat is, one in which we have split the data into two sets and used each in turn as a validation set.\n",
        "\n",
        "We could expand on this idea to use even more trials, and more folds in the data‚Äîfor example, here is a visual depiction of five-fold cross-validation:\n",
        "\n",
        "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.03-5-fold-CV.png?raw=1)\n",
        "\n",
        "Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data.\n",
        "This would be rather tedious to do by hand, and so we can use Scikit-Learn's ``cross_val_score`` convenience routine to do it succinctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "FRw7ENfsNOSj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(model, X, y, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UsfckjZANOSj"
      },
      "source": [
        "Repeating the validation across different subsets of the data gives us an even better idea of the performance of the algorithm.\n",
        "\n",
        "Scikit-Learn implements a number of useful cross-validation schemes that are useful in particular situations; these are implemented via iterators in the ``cross_validation`` module.\n",
        "For example, we might wish to go to the extreme case in which our number of folds is equal to the number of data points: that is, we train on all points but one in each trial.\n",
        "This type of cross-validation is known as *leave-one-out* cross validation, and can be used as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "k8dD8vX8NOSj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "scores = cross_val_score(model, X, y, cv=LeaveOneOut())\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "B4CwUK2pNOSj"
      },
      "source": [
        "Because we have 150 samples, the leave one out cross-validation yields scores for 150 trials, and the score indicates either successful (1.0) or unsuccessful (0.0) prediction.\n",
        "Taking the mean of these gives an estimate of the error rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BEJ9HBG8NOSj"
      },
      "outputs": [],
      "source": [
        "scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Wa2o_HB0NOSj"
      },
      "source": [
        "Other cross-validation schemes can be used similarly.\n",
        "For a description of what is available in Scikit-Learn, use IPython to explore the ``sklearn.cross_validation`` submodule, or take a look at Scikit-Learn's online [cross-validation documentation](http://scikit-learn.org/stable/modules/cross_validation.html)."
      ]
    }
  ]
}